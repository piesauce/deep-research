{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "id": "40d27939",
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1) Install dependencies\n"
      ],
      "metadata": {
        "id": "40d27939"
      }
    },
    {
      "id": "1ff83be0",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ff83be0",
        "outputId": "bd785964-3245-4846-9049-e724f75e13ae"
      },
      "execution_count": 2,
      "source": [
        "\n",
        "%%bash\n",
        "pip -q install --upgrade duckduckgo_search httpx trafilatura tiktoken json5 rich litellm tenacity python-dotenv pydantic==2.*\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.9/42.9 kB 4.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 8.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 462.4/462.4 kB 34.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 103.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.6/132.6 kB 16.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 243.4/243.4 kB 28.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 130.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 278.1/278.1 kB 31.7 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 837.9/837.9 kB 68.9 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 125.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 315.5/315.5 kB 32.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.7/274.7 kB 28.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.3 which is incompatible.\n",
            "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "id": "c14e2cf6",
      "cell_type": "code",
      "metadata": {
        "id": "c14e2cf6"
      },
      "execution_count": 3,
      "source": [
        "\n",
        "import os, re, math, time, json, json5, traceback, hashlib\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "from duckduckgo_search import DDGS\n",
        "import httpx\n",
        "import trafilatura\n",
        "from trafilatura.settings import use_config\n",
        "import tiktoken\n",
        "from pydantic import BaseModel, Field\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich import box\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load any keys injected in the environment (e.g., via Colab's secrets)\n",
        "load_dotenv()\n",
        "\n",
        "console = Console()\n",
        "\n",
        "# ---- Model selection (edit this) ----\n",
        "MODEL_NAME = os.environ.get(\"DEEP_RESEARCH_MODEL\", \"gpt-4o-mini\")  # Example: \"gpt-4o-mini\" or \"claude-3-5-sonnet-20241022\" or \"openrouter/auto\"\n",
        "\n",
        "# ---- Scratchpad budget (tokens) ----\n",
        "SCRATCHPAD_TOKEN_LIMIT = int(os.environ.get(\"SCRATCHPAD_TOKEN_LIMIT\", \"6000\"))\n",
        "\n",
        "# ---- Search parameters ----\n",
        "MAX_RESULTS_PER_QUERY = int(os.environ.get(\"MAX_RESULTS_PER_QUERY\", \"5\"))\n",
        "FETCH_TIMEOUT = float(os.environ.get(\"FETCH_TIMEOUT\", \"15.0\"))\n",
        "\n",
        "# ---- Final report parameters ----\n",
        "TARGET_REPORT_WORDS = int(os.environ.get(\"TARGET_REPORT_WORDS\", \"1200\"))\n",
        "\n",
        "# ---- Agent depth ----\n",
        "MAX_DEPTH = int(os.environ.get(\"MAX_DEPTH\", \"4\"))\n",
        "\n",
        "# ---- Safe search ----\n",
        "SAFESEARCH = os.environ.get(\"SAFESEARCH\", \"Moderate\")  # \"Off\", \"Moderate\", \"Strict\"\n",
        "REGION = os.environ.get(\"REGION\", \"us-en\")\n",
        "\n",
        "# ---- Provider abstraction via LiteLLM ----\n",
        "from litellm import completion\n",
        "\n",
        "def llm_generate(messages: List[Dict[str, str]],\n",
        "                 model: str = MODEL_NAME,\n",
        "                 temperature: float = 0.2,\n",
        "                 max_tokens: int = 1200) -> str:\n",
        "    \"\"\"Unified chat completion via LiteLLM (works with many providers).\n",
        "    Expects messages=[{\"role\":\"system\",\"content\":...}, {\"role\":\"user\",\"content\":...}, ...].\n",
        "    Returns the assistant text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = completion(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens)\n",
        "        # LiteLLM normalizes outputs\n",
        "        return resp.choices[0].message[\"content\"]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"LLM call failed: {e}\")\n",
        "\n",
        "\n",
        "# ---- Tokenization utilities ----\n",
        "def _get_encoder():\n",
        "    try:\n",
        "        return tiktoken.get_encoding(\"cl100k_base\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "_ENCODER = _get_encoder()\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    if not text:\n",
        "        return 0\n",
        "    if _ENCODER is None:\n",
        "        # Rough fallback: ~4 chars per token\n",
        "        return max(1, math.ceil(len(text) / 4))\n",
        "    return len(_ENCODER.encode(text))\n",
        "\n",
        "# ---- Robust JSON parsing ----\n",
        "def try_parse_json(s: str) -> Dict[str, Any]:\n",
        "    \"\"\"Try strict JSON, then JSON5, then bracket extraction.\"\"\"\n",
        "    if not s:\n",
        "        return {}\n",
        "    # direct json\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # json5\n",
        "    try:\n",
        "        return json5.loads(s)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # extract first {...}\n",
        "    try:\n",
        "        start = s.find('{')\n",
        "        end = s.rfind('}')\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            return json.loads(s[start:end+1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    # last resort: return as note\n",
        "    return {\"notes\": s.strip()}\n",
        "\n",
        "def hash_url(url: str) -> str:\n",
        "    return hashlib.sha1(url.encode(\"utf-8\")).hexdigest()[:10]\n",
        "\n",
        "# ---- Trafi config for better extraction ----\n",
        "trafi_cfg = use_config()\n",
        "trafi_cfg.set(\"DEFAULT\", \"EXTRACTION_TIMEOUT\", \"0\")   # let httpx handle timeouts\n",
        "trafi_cfg.set(\"DEFAULT\", \"EXTRACTION_TARGET_LANGUAGE\", \"en\")\n",
        "\n",
        "session = httpx.Client(timeout=FETCH_TIMEOUT, follow_redirects=True, headers={\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118 Safari/537.36\"\n",
        "})\n",
        "\n",
        "def fetch_content(url: str) -> Tuple[str, Optional[str]]:\n",
        "    \"\"\"Fetch URL and return (extracted_text, error).\"\"\"\n",
        "    try:\n",
        "        r = session.get(url)\n",
        "        r.raise_for_status()\n",
        "        downloaded = trafilatura.extract(r.text, url=url, config=trafi_cfg, include_comments=False, include_links=False)\n",
        "        if not downloaded:\n",
        "            # fallback: strip tags crudely\n",
        "            text = re.sub(r\"<[^>]+>\", \" \", r.text)\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        else:\n",
        "            text = downloaded\n",
        "        return text, None\n",
        "    except Exception as e:\n",
        "        return \"\", f\"{type(e).__name__}: {e}\"\n",
        "\n",
        "def ddg_search(query: str, max_results: int = MAX_RESULTS_PER_QUERY, region: str = REGION, safesearch: str = SAFESEARCH):\n",
        "    results = []\n",
        "    with DDGS() as ddgs:\n",
        "        for r in ddgs.text(query, region=region, safesearch=safesearch, max_results=max_results):\n",
        "            results.append({\n",
        "                \"title\": r.get(\"title\", \"\"),\n",
        "                \"href\": r.get(\"href\", \"\"),\n",
        "                \"body\": r.get(\"body\", \"\"),\n",
        "                \"source\": r.get(\"source\", \"\"),\n",
        "            })\n",
        "    return results\n",
        "\n",
        "class SearchDoc(BaseModel):\n",
        "    title: str\n",
        "    url: str\n",
        "    snippet: str\n",
        "    text: Optional[str] = None\n",
        "    id: str = Field(default_factory=lambda: f\"doc_{int(time.time()*1000)}\" )\n",
        "    score: float = 0.0\n",
        "\n",
        "class Scratchpad:\n",
        "    def __init__(self, token_limit: int = SCRATCHPAD_TOKEN_LIMIT):\n",
        "        self.token_limit = token_limit\n",
        "        self.entries: List[Dict[str, Any]] = []  # {\"depth\":int, \"notes\":str, \"citations\":List[str]}\n",
        "        self.total_tokens = 0\n",
        "\n",
        "    def add(self, depth: int, notes: str, citations: List[str] = None):\n",
        "        citations = citations or []\n",
        "        tokens = count_tokens(notes)\n",
        "        self.entries.append({\"depth\": depth, \"notes\": notes, \"citations\": citations, \"tokens\": tokens})\n",
        "        self.total_tokens += tokens\n",
        "\n",
        "    def to_text(self) -> str:\n",
        "        chunks = []\n",
        "        for i, e in enumerate(self.entries, 1):\n",
        "            cites = \" \".join(f\"[{c}]\" for c in e.get(\"citations\", []))\n",
        "            chunks.append(f\"[d={e['depth']}] {e['notes']} {cites}\".strip())\n",
        "        return \"\\n\\n\".join(chunks)\n",
        "\n",
        "    def _oldest_block_indexes(self, take: int = 4) -> List[int]:\n",
        "        # Take a small window from the oldest entries for summarization\n",
        "        return list(range(0, min(take, len(self.entries))))\n",
        "\n",
        "    def over_budget(self) -> bool:\n",
        "        return self.total_tokens > self.token_limit\n",
        "\n",
        "    def entries_token_count(self, idxs: List[int]) -> int:\n",
        "        return sum(self.entries[i][\"tokens\"] for i in idxs)\n",
        "\n",
        "    def remove_and_insert_summary(self, idxs: List[int], summary: str):\n",
        "        # Remove old entries and add a summarized one\n",
        "        idxs_sorted = sorted(idxs, reverse=True)\n",
        "        removed_tokens = 0\n",
        "        citations = []\n",
        "        for i in idxs_sorted:\n",
        "            removed_tokens += self.entries[i][\"tokens\"]\n",
        "            citations.extend(self.entries[i].get(\"citations\", []))\n",
        "            self.entries.pop(i)\n",
        "        self.total_tokens -= removed_tokens\n",
        "        self.add(depth=-1, notes=f\"[Summarized] {summary}\", citations=list(sorted(set(citations))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "class ResearchState(BaseModel):\n",
        "    question: str\n",
        "    depth: int = 0\n",
        "    seen_urls: set = Field(default_factory=set)\n",
        "    docs: Dict[str, SearchDoc] = Field(default_factory=dict)\n",
        "    report_chunks: List[str] = Field(default_factory=list)\n",
        "    citations: Dict[str, str] = Field(default_factory=dict)  # id -> url\n",
        "    no_new_queries_rounds: int = 0\n",
        "\n",
        "# ---- Orchestrator ----\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a meticulous research analyst. You follow this loop:\n",
        "1) Read the research question and the provided search snippets.\n",
        "2) Write compact, factual notes into a scratchpad (bullet points or short paragraphs).\n",
        "3) Propose 3-6 NEXT web search queries to go deeper **top-down** (broad to narrow, then to key sub-questions).\n",
        "4) Decide whether we can stop. Stop only if the final report is coherent, sourced, and covers the key sub-questions.\n",
        "\n",
        "You must output STRICT JSON with these keys:\n",
        "{\n",
        "  \"notes\": \"short notes capturing what matters (with inline source ids like [doc_abc123])\",\n",
        "  \"next_queries\": [\"...\"],\n",
        "  \"final_report_chunk\": \"<= 200 words coherent draft chunk with inline [doc_id] citations when appropriate\",\n",
        "  \"should_stop\": false,\n",
        "  \"confidence\": 0.0\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "SUMMARIZE_PROMPT = \"\"\"Summarize the following scratchpad notes into a compact, loss-aware synthesis that preserves specific facts and internal references.\n",
        "Return **plain text** under 250 words.\n",
        "\n",
        "NOTES:\n",
        "{notes}\n",
        "\"\"\"\n",
        "\n",
        "INITIAL_QUERY_PROMPT = \"\"\"You are planning top-down research for the user question below.\n",
        "Propose 4-6 broad, high-yield web search queries that map the landscape. Return **JSON**: {{\"next_queries\": [\"...\"]}}.\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_md_citation(url: str, doc_id: str) -> str:\n",
        "    return f\"[{doc_id}]({url})\"\n",
        "\n",
        "def collect_context_text(docs: Dict[str, SearchDoc], limit_chars: int = 6000) -> str:\n",
        "    # Build a compact context bundle of titles/snippets/text excerpts\n",
        "    items = []\n",
        "    running = 0\n",
        "    for doc_id, d in docs.items():\n",
        "        snippet = d.text or d.snippet or \"\"\n",
        "        piece = f\"{doc_id}: {d.title}\\nURL: {d.url}\\nEXCERPT: {snippet[:800]}\\n\\n\"\n",
        "        l = len(piece)\n",
        "        if running + l > limit_chars:\n",
        "            break\n",
        "        items.append(piece)\n",
        "        running += l\n",
        "    return \"\".join(items)\n",
        "\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_random_exponential(min=1, max=4))\n",
        "def reason_once(question: str, docs: Dict[str, SearchDoc], scratchpad: Scratchpad) -> Dict[str, Any]:\n",
        "    context = collect_context_text(docs)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"Question:\\n{question}\\n\\nSearch context:\\n{context}\\n\\nScratchpad so far:\\n{scratchpad.to_text()}\"}\n",
        "    ]\n",
        "    out = llm_generate(messages, model=MODEL_NAME, temperature=0.2, max_tokens=1100)\n",
        "    return try_parse_json(out)\n",
        "\n",
        "def propose_initial_queries(question: str) -> List[str]:\n",
        "    out = llm_generate(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You help write only JSON.\"},\n",
        "            {\"role\": \"user\", \"content\": INITIAL_QUERY_PROMPT.format(question=question)}\n",
        "        ],\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.3,\n",
        "        max_tokens=300,\n",
        "    )\n",
        "    j = try_parse_json(out)\n",
        "    qs = j.get(\"next_queries\") or []\n",
        "    # fallback\n",
        "    if not qs:\n",
        "        qs = [f\"Overview of {question}\", f\"Key studies about {question}\", f\"Latest developments in {question}\"]\n",
        "    return qs[:6]\n",
        "\n",
        "def summarize_block(llm_text: str) -> str:\n",
        "    return llm_generate(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You write compact, loss-aware summaries.\"},\n",
        "            {\"role\": \"user\", \"content\": SUMMARIZE_PROMPT.format(notes=llm_text)}\n",
        "        ],\n",
        "        model=MODEL_NAME,\n",
        "        temperature=0.2,\n",
        "        max_tokens=300,\n",
        "    ).strip()\n",
        "\n",
        "def recursively_summarize_if_needed(scratchpad: Scratchpad):\n",
        "    # While over budget: compress a window from the oldest entries\n",
        "    guard = 0\n",
        "    while scratchpad.over_budget() and guard < 10 and len(scratchpad) > 2:\n",
        "        idxs = scratchpad._oldest_block_indexes(take= min(6, len(scratchpad)//2))\n",
        "        notes_text = \"\\n\\n\".join(scratchpad.entries[i][\"notes\"] for i in idxs)\n",
        "        summary = summarize_block(notes_text)\n",
        "        scratchpad.remove_and_insert_summary(idxs, summary)\n",
        "        guard += 1\n",
        "        console.log(f\"[yellow]Scratchpad over budget → summarized {len(idxs)} entries.\")\n",
        "\n",
        "\n",
        "def run_search_round(queries: List[str], state: ResearchState):\n",
        "    new_docs = 0\n",
        "    for q in queries:\n",
        "        results = ddg_search(q, max_results=MAX_RESULTS_PER_QUERY)\n",
        "        for r in results:\n",
        "            url = r.get(\"href\") or r.get(\"url\") or \"\"\n",
        "            if not url or url in state.seen_urls:\n",
        "                continue\n",
        "            title = r.get(\"title\", \"\").strip() or url\n",
        "            snippet = r.get(\"body\", \"\").strip()\n",
        "            text, err = fetch_content(url)\n",
        "            doc_id = f\"doc_{hash_url(url)}\"\n",
        "            state.docs[doc_id] = SearchDoc(\n",
        "                title=title, url=url, snippet=snippet, text=text[:6000] if text else \"\"\n",
        "            )\n",
        "            state.citations[doc_id] = url\n",
        "            state.seen_urls.add(url)\n",
        "            new_docs += 1\n",
        "    return new_docs\n",
        "\n",
        "def pretty_results_table(state: ResearchState, top_k: int = 10) -> Table:\n",
        "    t = Table(title=\"Fetched documents\", box=box.SIMPLE_HEAVY)\n",
        "    t.add_column(\"doc_id\", style=\"bold\"), t.add_column(\"Title\"), t.add_column(\"URL\")\n",
        "    for i, (doc_id, d) in enumerate(list(state.docs.items())[:top_k]):\n",
        "        t.add_row(doc_id, d.title[:80], d.url[:80])\n",
        "    return t\n",
        "\n",
        "def render_report(report_chunks: List[str], citations: Dict[str, str]) -> str:\n",
        "    report = \"\\n\\n\".join(report_chunks).strip()\n",
        "    # Expand inline [doc_id] to markdown links\n",
        "    def repl(m):\n",
        "        doc_id = m.group(1)\n",
        "        url = citations.get(doc_id, None)\n",
        "        return f\"[{doc_id}]({url})\" if url else f\"[{doc_id}]\"\n",
        "    report = re.sub(r\"\\[(doc_[a-zA-Z0-9]+)\\]\", repl, report)\n",
        "    return report\n",
        "\n",
        "def save_artifacts(question: str, scratchpad: Scratchpad, state: ResearchState, out_dir: str = \"artifacts\") -> Dict[str,str]:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    scratch_path = os.path.join(out_dir, \"scratchpad.json\")\n",
        "    session_path = os.path.join(out_dir, \"session.json\")\n",
        "    report_path = os.path.join(out_dir, \"report.md\")\n",
        "\n",
        "    with open(scratch_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"token_limit\": scratchpad.token_limit, \"total_tokens\": scratchpad.total_tokens, \"entries\": scratchpad.entries}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(session_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"question\": question,\n",
        "            \"depth\": state.depth,\n",
        "            \"docs\": {k: v.model_dump() for k, v in state.docs.items()},\n",
        "            \"citations\": state.citations,\n",
        "            \"report_chunks\": state.report_chunks,\n",
        "        }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    report_md = render_report(state.report_chunks, state.citations)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"# Research Report\\n\\n**Question:** {question}\\n\\n{report_md}\\n\")\n",
        "    return {\"scratchpad\": scratch_path, \"session\": session_path, \"report\": report_path}\n",
        "\n",
        "def deep_research(question: str,\n",
        "                  max_depth: int = MAX_DEPTH,\n",
        "                  target_report_words: int = TARGET_REPORT_WORDS,\n",
        "                  scratchpad_token_limit: int = SCRATCHPAD_TOKEN_LIMIT) -> Dict[str, Any]:\n",
        "    console.rule(f\"[bold cyan]Deep Research: {question}\")\n",
        "    scratchpad = Scratchpad(token_limit=scratchpad_token_limit)\n",
        "    state = ResearchState(question=question)\n",
        "\n",
        "    # 0) Initial top-down queries\n",
        "    init_queries = propose_initial_queries(question)\n",
        "    console.print(Panel.fit(\"\\n\".join(f\"• {q}\" for q in init_queries), title=\"Initial queries\", border_style=\"cyan\"))\n",
        "    added = run_search_round(init_queries, state)\n",
        "    console.print(pretty_results_table(state))\n",
        "    if added == 0:\n",
        "        console.print(\"[yellow]No documents found in the first round. You may want to try rephrasing the question.\")\n",
        "\n",
        "    # Main loop\n",
        "    while True:\n",
        "        state.depth += 1\n",
        "        console.rule(f\"Depth {state.depth}\")\n",
        "\n",
        "        # 1) Reason over current evidence\n",
        "        try:\n",
        "            step = reason_once(question, state.docs, scratchpad)\n",
        "        except Exception as e:\n",
        "            console.print(f\"[red]Reasoning failed:[/red] {e}\\n{traceback.format_exc()}\")\n",
        "            break\n",
        "\n",
        "        notes = step.get(\"notes\", \"\").strip()\n",
        "        if notes:\n",
        "            # Extract inline [doc_id] refs to keep citations\n",
        "            cites = re.findall(r\"\\[(doc_[a-zA-Z0-9]+)\\]\", notes)\n",
        "            scratchpad.add(state.depth, notes, citations=cites)\n",
        "            recursively_summarize_if_needed(scratchpad)\n",
        "\n",
        "        # 2) Update report\n",
        "        chunk = step.get(\"final_report_chunk\", \"\").strip()\n",
        "        if chunk:\n",
        "            state.report_chunks.append(chunk)\n",
        "\n",
        "        # 3) Next queries\n",
        "        queries = [q for q in (step.get(\"next_queries\") or []) if q and isinstance(q, str)]\n",
        "        if queries:\n",
        "            new_docs = run_search_round(queries, state)\n",
        "            if new_docs == 0:\n",
        "                state.no_new_queries_rounds += 1\n",
        "            else:\n",
        "                state.no_new_queries_rounds = 0\n",
        "        else:\n",
        "            state.no_new_queries_rounds += 1\n",
        "\n",
        "        # 4) Termination checks\n",
        "        words_so_far = len(re.findall(r\"\\w+\", render_report(state.report_chunks, state.citations)))\n",
        "        should_stop = bool(step.get(\"should_stop\")) or state.depth >= max_depth or state.no_new_queries_rounds >= 2 or words_so_far >= target_report_words\n",
        "\n",
        "        console.print(Panel.fit(\n",
        "            f\"Report words: {words_so_far} / {target_report_words}\\nScratchpad tokens: {scratchpad.total_tokens} / {scratchpad.token_limit}\\nNew docs this round: {0 if not queries else new_docs}\\nNo-new-query rounds: {state.no_new_queries_rounds}\\nShould stop (model): {bool(step.get('should_stop'))}\",\n",
        "            title=\"Progress\", border_style=\"magenta\"))\n",
        "\n",
        "        if should_stop:\n",
        "            console.print(\"[green]Stopping criteria met.\")\n",
        "            break\n",
        "\n",
        "    # Save artifacts\n",
        "    paths = save_artifacts(question, scratchpad, state, out_dir=\"artifacts\")\n",
        "    console.print(Panel.fit(\"Saved artifacts:\\n- \" + \"\\n- \".join(f\"{k}: {v}\" for k, v in paths.items()),\n",
        "                            title=\"Artifacts\", border_style=\"green\"))\n",
        "    return {\n",
        "        \"report_md\": render_report(state.report_chunks, state.citations),\n",
        "        \"artifacts\": paths\n",
        "    }\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "QUESTION = \"What are the most promising approaches to decarbonizing long-haul aviation by 2040, and what are the key technical and economic constraints?\"\n",
        "\n",
        "result = deep_research(QUESTION, max_depth=MAX_DEPTH, target_report_words=TARGET_REPORT_WORDS, scratchpad_token_limit=SCRATCHPAD_TOKEN_LIMIT)\n",
        "\n",
        "print(\"\\n\\n===== FINAL REPORT (Markdown) =====\\n\")\n",
        "print(result[\"report_md\"])\n",
        "\n",
        "print(\"\\n\\nFiles saved:\")\n",
        "for k, v in result[\"artifacts\"].items():\n",
        "    print(f\"- {k}: {v}\")"
      ],
      "metadata": {
        "id": "8eJCQjnbKYJl"
      },
      "id": "8eJCQjnbKYJl",
      "execution_count": null,
      "outputs": []
    }
  ]
}